model_name: llama_8b_qlora_uncensored
base_model: meta-llama/Meta-Llama-3-8B-Instruct
model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
target_modules:  # modules for which to train lora adapters
- q_proj
- k_proj
- v_proj
- o_proj
#- gate_proj
#- up_proj
#- down_proj
#dataset: ehartford/wizard_vicuna_70k_unfiltered
trainer_output_dir: trainer_outputs/
model_output_dir: models/  # model saved in {model_output_dir}/{model_name}
instruct: false  # train for instruct (true) or chat (false)